"""
LangGraph å®ç°çš„ React Agent
æ­¤å®ç°ä¿æŒäº†åŸå§‹ react_agent.py çš„æ‰€æœ‰åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- Token è®¡æ•°å’Œé™åˆ¶æ£€æŸ¥
- ç‰¹æ®Šå·¥å…·è°ƒç”¨å¤„ç† (<tool_call>, <tool_response>)
- ä¸­é—´ç»“æœçš„æµå¼è¾“å‡º
- è‡ªå®šä¹‰é‡è¯•é€»è¾‘å’Œé”™è¯¯å¤„ç†
- A2A åè®®æ”¯æŒçš„æµå¼æ¥å£
"""

import json
import json5
import os
import time
import random
import asyncio
from typing import Dict, List, Optional, TypedDict, Annotated, AsyncIterable, Any
from datetime import datetime
from pathlib import Path

# åœ¨å¯¼å…¥å·¥å…·ä¹‹å‰åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆå·¥å…·æ¨¡å—åœ¨å¯¼å…¥æ—¶ä¼šè¯»å–ç¯å¢ƒå˜é‡ï¼‰
from dotenv import load_dotenv

# å°è¯•ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½ .env æ–‡ä»¶
SCRIPT_DIR = Path(__file__).resolve().parent
ENV_FILE = SCRIPT_DIR.parent / ".env"

if ENV_FILE.exists():
    load_dotenv(ENV_FILE)
    print(f"âœ… å·²åŠ è½½ç¯å¢ƒå˜é‡: {ENV_FILE}")
else:
    # å¦‚æœæ ¹ç›®å½•æ²¡æœ‰ .envï¼Œå°è¯•å½“å‰ç›®å½•
    load_dotenv()
    print("âš ï¸  æœªæ‰¾åˆ° .env æ–‡ä»¶ï¼Œä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡")

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from openai import OpenAI, APIError, APIConnectionError, APITimeoutError
from transformers import AutoTokenizer

from prompt import SYSTEM_PROMPT
from tool_file import FileParser
from tool_scholar import Scholar
from tool_python import PythonInterpreter
from tool_search import Search
from tool_visit import Visit


# å¸¸é‡å®šä¹‰
OBS_START = '<tool_response>'
OBS_END = '\n</tool_response>'
MAX_LLM_CALL_PER_RUN = int(os.getenv('MAX_LLM_CALL_PER_RUN', 100))
MAX_TOKENS_LIMIT = 110 * 1024
MAX_TIME_LIMIT = 150 * 60  # 150åˆ†é’Ÿï¼ˆç§’ï¼‰

# åˆå§‹åŒ–å·¥å…·
TOOL_CLASS = [
    FileParser(),
    Scholar(),
    Visit(),
    Search(),
    PythonInterpreter(),
]
TOOL_MAP = {tool.name: tool for tool in TOOL_CLASS}


def today_date():
    """è·å–ä»Šå¤©çš„æ—¥æœŸï¼Œæ ¼å¼ä¸º YYYY-MM-DD"""
    return datetime.date.today().strftime("%Y-%m-%d")


class AgentState(TypedDict):
    """LangGraph agent çš„çŠ¶æ€å®šä¹‰"""
    messages: List[Dict]  # å¯¹è¯å†å²
    question: str  # åŸå§‹é—®é¢˜
    answer: str  # æ ‡å‡†ç­”æ¡ˆï¼ˆç”¨äºè¯„ä¼°,æš‚æ—¶ä¸ç”¨ï¼‰
    prediction: str  # Agent çš„é¢„æµ‹ç­”æ¡ˆ
    termination: str  # ç»ˆæ­¢åŸå› 
    round: int  # å½“å‰è½®æ¬¡
    num_llm_calls_available: int  # å‰©ä½™ LLM è°ƒç”¨æ¬¡æ•°
    start_time: float  # å¼€å§‹æ—¶é—´æˆ³

    # å·¥å…·ç›¸å…³å­—æ®µ
    tool_name: Optional[str]  # å½“å‰å·¥å…·åç§°
    tool_args: Optional[Dict]  # å½“å‰å·¥å…·å‚æ•°
    tool_result: Optional[str]  # å·¥å…·ç»“æœæˆ–é”™è¯¯æ¶ˆæ¯
    
    # ç»ˆæ­¢æ§åˆ¶å­—æ®µ
    should_terminate: bool  # æ˜¯å¦ç»ˆæ­¢
    termination_reason: Optional[str]  # ç»ˆæ­¢åŸå› 
    need_final_answer: bool  # æ˜¯å¦éœ€è¦æœ€ç»ˆç­”æ¡ˆ

class LangGraphReactAgent:
    """
    åŸºäº LangGraph çš„ React Agentï¼Œæ”¯æŒæµå¼è¾“å‡ºå’Œè‡ªå®šä¹‰å·¥å…·å¤„ç†
    åŒæ—¶æ”¯æŒ A2A åè®®çš„æµå¼å“åº”
    """
    
    # A2A åè®®æ”¯æŒçš„å†…å®¹ç±»å‹
    SUPPORTED_CONTENT_TYPES = ['text']
    
    def __init__(self, llm_config: Optional[Dict] = None, model_path: Optional[str] = None):
        """
        åˆå§‹åŒ– LangGraph React Agent
        
        Args:
            llm_config: LLM ç”Ÿæˆé…ç½®ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆç”¨äº tokenizerï¼‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        """
        # å¦‚æœæ²¡æœ‰æä¾› llm_configï¼Œä»ç¯å¢ƒå˜é‡è¯»å–
        if llm_config is None:
            llm_config = {
                'temperature': float(os.getenv('TEMPERATURE', '0.85')),
                'top_p': float(os.getenv('TOP_P', '0.95')),
                'presence_penalty': float(os.getenv('PRESENCE_PENALTY', '1.1')),
            }
            print(f"ğŸ“‹ ä½¿ç”¨é»˜è®¤ LLM é…ç½®: {llm_config}")
        
        # å¦‚æœæ²¡æœ‰æä¾› model_pathï¼Œä»ç¯å¢ƒå˜é‡è¯»å–
        if model_path is None:
            model_path = os.getenv('MODEL_PATH', '../models')
            print(f"ğŸ“ ä½¿ç”¨é»˜è®¤æ¨¡å‹è·¯å¾„: {model_path}")
        
        self.llm_config = llm_config
        self.llm_local_path = model_path
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # æ„å»ºå›¾
        self.graph = self._build_graph()
        
    def _build_graph(self) -> StateGraph:
        """
        æ„å»º LangGraph çŠ¶æ€å›¾ï¼ŒåŒ…å«ä»¥ä¸‹èŠ‚ç‚¹ï¼š
        1. LLM ç”Ÿæˆ
        2. å·¥å…·æ‰§è¡Œ
        3. ç­”æ¡ˆæå–
        4. ç»ˆæ­¢æ£€æŸ¥
        """
        workflow = StateGraph(AgentState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("llm_call", self._llm_call_node)
        workflow.add_node("parse_response", self._parse_response_node)
        workflow.add_node("execute_tool", self._execute_tool_node)
        workflow.add_node("check_termination", self._check_termination_node)
        workflow.add_node("extract_answer", self._extract_answer_node)
        
        # å®šä¹‰è¾¹
        workflow.set_entry_point("llm_call")
        
        workflow.add_edge("llm_call", "parse_response")
        
        workflow.add_conditional_edges(
            "parse_response",
            self._should_execute_tool,
            {
                "execute_tool": "execute_tool",
                "check_termination": "check_termination"
            }
        )
        
        workflow.add_edge("execute_tool", "check_termination")
        
        workflow.add_conditional_edges(
            "check_termination",
            self._should_continue,
            {
                "llm_call": "llm_call",
                "extract_answer": "extract_answer",
                "end": END
            }
        )
        
        workflow.add_edge("extract_answer", END)
        
        return workflow.compile()
    
    def _count_tokens(self, messages: List[Dict]) -> int:
        """è®¡ç®—æ¶ˆæ¯å†å²ä¸­çš„ token æ•°é‡"""
        full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False)
        tokens = self.tokenizer(full_prompt, return_tensors="pt")
        return len(tokens["input_ids"][0])
    
    def _call_llm_with_retry(self, messages: List[Dict], max_tries: int = 5) -> str:
        """
        è°ƒç”¨ LLM APIï¼Œé‡‡ç”¨æŒ‡æ•°é€€é¿é‡è¯•é€»è¾‘
        
        Args:
            messages: å¯¹è¯å†å²
            max_tries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            LLM ç”Ÿæˆçš„å†…å®¹
        """
        
        openai_api_key = os.getenv("API_KEY", "your-openai-api-key")
        openai_api_base = os.getenv("API_BASE", "http://127.0.0.1:1234/v1")

        client = OpenAI(
            api_key=openai_api_key,
            base_url=openai_api_base,
            timeout=600.0,
        )
        
        base_sleep_time = 1
        for attempt in range(max_tries):
            try:
                print(f"--- å°è¯•è°ƒç”¨ LLMï¼Œç¬¬ {attempt + 1}/{max_tries} æ¬¡ ---")
                
                chat_response = client.chat.completions.create(
                    model='alibaba-nlp_tongyi-deepresearch-30b-a3b@q5_k_l',
                    messages=messages,
                    stream=True,
                    stop=["\n<tool_response>", "<tool_response>"],
                    temperature=self.llm_config.get('temperature', 0.6),
                    top_p=self.llm_config.get('top_p', 0.95),
                    logprobs=True,
                    max_tokens=10000
                )
                
                #non-streaming
                #content = chat_response.choices[0].message.content
                
                content = ""
                for chunk in chat_response:
                    if chunk.choices[0].delta.content:
                        content += chunk.choices[0].delta.content
                        print(chunk.choices[0].delta.content, end='', flush=True)

                # OpenRouter
                #reasoning_content = "<think>\n" + (
                #   chat_response.choices[0].message.reasoning.strip()
                #   if chat_response.choices[0].message.reasoning
                #   else ""
                #) + "\n</think>"
                #content = reasoning_content + content
                
                if content and content.strip():
                    print("\n--- LLM è°ƒç”¨æˆåŠŸ ---")
                    return content.strip()
                else:
                    print(f"è­¦å‘Š: ç¬¬ {attempt + 1} æ¬¡å°è¯•æ”¶åˆ°ç©ºå“åº”")
                    
            except (APIError, APIConnectionError, APITimeoutError) as e:
                print(f"é”™è¯¯: ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ï¼ŒAPI é”™è¯¯: {e}")
            except Exception as e:
                print(f"é”™è¯¯: ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ï¼Œæœªé¢„æœŸé”™è¯¯: {e}")
            
            if attempt < max_tries - 1:
                sleep_time = base_sleep_time * (2 ** attempt) + random.uniform(0, 1)
                sleep_time = min(sleep_time, 30)
                print(f"ç­‰å¾… {sleep_time:.2f} ç§’åé‡è¯•...")
                time.sleep(sleep_time)
            else:
                print("é”™è¯¯: æ‰€æœ‰é‡è¯•å°è¯•å·²ç”¨å°½")
        
        return "vllm server error!!!"
    
    def _execute_tool_call(self, tool_name: str, tool_args: dict) -> str:
        """
        æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼Œå¸¦æœ‰é€‚å½“çš„é”™è¯¯å¤„ç†
        
        Args:
            tool_name: è¦æ‰§è¡Œçš„å·¥å…·åç§°
            tool_args: å·¥å…·å‚æ•°
            
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœå­—ç¬¦ä¸²
        """
        if tool_name not in TOOL_MAP:
            return f"é”™è¯¯: å·¥å…· {tool_name} æœªæ‰¾åˆ°"
        
        try:
            tool_args["params"] = tool_args
            
            if "python" in tool_name.lower():
                result = TOOL_MAP['PythonInterpreter'].call(tool_args)
            elif tool_name == "parse_file":
                params = {"files": tool_args.get("files", [])}
                raw_result = asyncio.run(
                    TOOL_MAP[tool_name].call(params, file_root_path="./eval_data/file_corpus")
                )
                result = str(raw_result) if not isinstance(raw_result, str) else raw_result
            else:
                raw_result = TOOL_MAP[tool_name].call(tool_args)
                result = raw_result
                
            return result
        except Exception as e:
            return f"æ‰§è¡Œå·¥å…· {tool_name} æ—¶å‡ºé”™: {str(e)}"
    
    def _llm_call_node(self, state: AgentState) -> AgentState:
        """
        èŠ‚ç‚¹: è°ƒç”¨ LLM ç”Ÿæˆä¸‹ä¸€ä¸ªå“åº”
        æ­¤èŠ‚ç‚¹é€šè¿‡äº§ç”Ÿä¸­é—´ç»“æœæ¥å¤„ç†æµå¼è¾“å‡º
        """
        print(f"\n=== ç¬¬ {state['round'] + 1} è½®æ¨ç† ===")
        
        # å¢åŠ è½®æ¬¡è®¡æ•°å™¨
        state["round"] += 1
        state["num_llm_calls_available"] -= 1
        
        # è°ƒç”¨ LLM
        #print(f"current messages: {state['messages']}")
        content = self._call_llm_with_retry(state["messages"])

        
        # ä»å†…å®¹ä¸­ç§»é™¤ä»»ä½• tool_response æ ‡ç­¾
        if '<tool_response>' in content:
            pos = content.find('<tool_response>')
            content = content[:pos]
        
        # å°†åŠ©æ‰‹å“åº”æ·»åŠ åˆ°æ¶ˆæ¯ä¸­
        state["messages"].append({
            "role": "assistant",
            "content": content.strip()
        })
        
        return state
    
    def _parse_response_node(self, state: AgentState) -> AgentState:
        """
        èŠ‚ç‚¹: è§£æ LLM å“åº”ä»¥æå–å·¥å…·è°ƒç”¨æˆ–ç­”æ¡ˆ
        """
        print("\n=== è§£æå“åº” ===")
        
        content = state["messages"][-1]["content"]
        
        # æ£€æŸ¥å·¥å…·è°ƒç”¨
        if '<tool_call>' in content and '</tool_call>' in content:
            tool_call_str = content.split('<tool_call>')[1].split('</tool_call>')[0].strip()
            
            # å¤„ç† Python è§£é‡Šå™¨ç‰¹æ®Šæƒ…å†µ
            if "python" in tool_call_str.lower() and "<code>" in tool_call_str:
                try:
                    code_raw = content.split('<tool_call>')[1].split('</tool_call>')[0]
                    code_raw = code_raw.split('<code>')[1].split('</code>')[0].strip()
                    state["tool_name"] = "PythonInterpreter"
                    state["tool_args"] = {"code": code_raw}
                    print(f"è§£æå·¥å…·: {state['tool_name']}")
                except Exception as e:
                    print(f"Python è§£æé”™è¯¯: {e}")
                    state["tool_result"] = "[Python Interpreter Error]: Formatting error."
                    state["tool_name"] = None
                    
            else:
                try:
                    tool_call = json5.loads(tool_call_str)
                    state["tool_name"] = tool_call.get('name', '')
                    state["tool_args"] = tool_call.get('arguments', {})
                    print(f"è§£æå·¥å…·: {state['tool_name']}")
                except Exception as e:
                    print(f"JSON è§£æé”™è¯¯: {e}")
                    state["tool_result"] = 'Error: Tool call is not a valid JSON. Tool call must contain a valid "name" and "arguments" field.'
                    state["tool_name"] = None
        
        return state
    
    def _should_execute_tool(self, state: AgentState) -> str:
        """
        æ¡ä»¶è¾¹: ç¡®å®šæ˜¯å¦åº”è¯¥æ‰§è¡Œå·¥å…·
        """
        if state.get("tool_name"):
            return "execute_tool"
        return "check_termination"
    
    def _execute_tool_node(self, state: AgentState) -> AgentState:
        """
        èŠ‚ç‚¹: æ‰§è¡Œå·²è§£æçš„å·¥å…·è°ƒç”¨
        """
        tool_name = state.get("tool_name")
        tool_args = state.get("tool_args", {})
        
        print(f"\n=== æ‰§è¡Œå·¥å…·: {tool_name} ===")
        
        if state.get("tool_result"):
            # é”™è¯¯å·²åœ¨è§£æèŠ‚ç‚¹ä¸­è®¾ç½®
            result = state["tool_result"]
        elif tool_name == "PythonInterpreter":
            try:
                result = TOOL_MAP['PythonInterpreter'].call(tool_args.get("code", ""))
            except Exception as e:
                result = f"[Python Interpreter Error]: {str(e)}"
        else:
            result = self._execute_tool_call(tool_name, tool_args)
        
        # ä½¿ç”¨ tool_response æ ‡ç­¾æ ¼å¼åŒ–ç»“æœ
        result = f"<tool_response>\n{result}\n</tool_response>"
        
        # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
        state["messages"].append({
            "role": "user",
            "content": result
        })
        
        # æ¸…ç†å·¥å…·ç›¸å…³çŠ¶æ€
        state["tool_name"] = None
        state["tool_args"] = None
        state["tool_result"] = None
        
        return state
    
    def _check_termination_node(self, state: AgentState) -> AgentState:
        """
        èŠ‚ç‚¹: æ£€æŸ¥å„ç§ç»ˆæ­¢æ¡ä»¶
        """
        print("\n=== æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶ ===")
        
        content = state["messages"][-1]["content"]
        
        # æ£€æŸ¥æ˜¯å¦æä¾›äº†ç­”æ¡ˆ
        if '<answer>' in content and '</answer>' in content:
            state["should_terminate"] = True
            state["termination_reason"] = "answer"
            state["need_final_answer"] = False  
            print("âœ“ æ£€æµ‹åˆ°ç­”æ¡ˆæ ‡ç­¾")
            return state
        
        # æ£€æŸ¥æ—¶é—´é™åˆ¶
        elapsed_time = time.time() - state["start_time"]
        if elapsed_time > MAX_TIME_LIMIT:
            state["should_terminate"] = True
            state["termination_reason"] = "time_limit"
            state["prediction"] = "No answer found after 2h30mins"
            state["termination"] = "No answer found after 2h30mins"
            print(f"âœ— è¶…æ—¶: {elapsed_time:.0f}ç§’")
            return state
        
        # æ£€æŸ¥ LLM è°ƒç”¨æ¬¡æ•°é™åˆ¶
        if state["num_llm_calls_available"] <= 0:
            state["messages"][-1]["content"] = "Sorry, the number of llm calls exceeds the limit."
            state["should_terminate"] = True
            state["termination_reason"] = "llm_call_limit"
            print("âœ— LLM è°ƒç”¨æ¬¡æ•°è¶…é™")
            return state
        
        # æ£€æŸ¥ token é™åˆ¶
        token_count = self._count_tokens(state["messages"])
        print(f"Token æ•°é‡: {token_count:,}")
        
        if token_count > MAX_TOKENS_LIMIT:
            print(f"âœ— Token è¶…é™: {token_count:,} > {MAX_TOKENS_LIMIT:,}")
            
            # æ·»åŠ æœ€ç»ˆæç¤ºä»¥ç”Ÿæˆç­”æ¡ˆ
            state["messages"][-1]["content"] = (
                "You have now reached the maximum context length you can handle. "
                "You should stop making tool calls and, based on all the information above, "
                "think again and provide what you consider the most likely answer in the "
                "following format:<think>your final thinking</think>\n<answer>your answer</answer>"
            )
            
            state["should_terminate"] = True
            state["termination_reason"] = "token_limit"
            state["need_final_answer"] = True
            return state
        
        # å¦‚æœæ²¡æœ‰æ»¡è¶³ç»ˆæ­¢æ¡ä»¶ï¼Œåˆ™ç»§ç»­
        state["should_terminate"] = False
        state["termination_reason"] = None
        
        return state
    
    def _should_continue(self, state: AgentState) -> str:
        """
        æ¡ä»¶è¾¹: åŸºäºç»ˆæ­¢æ£€æŸ¥ç¡®å®šä¸‹ä¸€æ­¥
        """
        if state.get("should_terminate"):
            # ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼ˆtoken è¶…é™çš„æƒ…å†µï¼‰
            if state.get("need_final_answer"):
                return "llm_call"
            # æ£€æµ‹åˆ°ç­”æ¡ˆæ ‡ç­¾ï¼Œç›´æ¥æå–ç­”æ¡ˆ
            elif state.get("termination_reason") == "answer":
                return "extract_answer"
            # LLM è°ƒç”¨æ¬¡æ•°è¶…é™æˆ–æ—¶é—´è¶…é™ï¼Œç›´æ¥ç»“æŸ
            elif state.get("termination_reason") in ["llm_call_limit", "time_limit"]:
                return "end"
            else:
                return "end"
        return "llm_call"
    
    def _extract_answer_node(self, state: AgentState) -> AgentState:
        """
        èŠ‚ç‚¹: ä»å“åº”ä¸­æå–æœ€ç»ˆç­”æ¡ˆ
        """
        print("\n=== æå–æœ€ç»ˆç­”æ¡ˆ ===")
        
        content = state["messages"][-1]["content"]
        
        if '<answer>' in content and '</answer>' in content:
            prediction = content.split('<answer>')[1].split('</answer>')[0]
            termination = state.get("termination_reason", "answer")
            
            if state.get("need_final_answer"):
                termination = "generate an answer as token limit reached"
        else:
            prediction = content if state.get("need_final_answer") else "No answer found."
            termination = "format error" if state.get("need_final_answer") else "answer not found"
            
            if state["num_llm_calls_available"] == 0:
                termination = "exceed available llm calls"
        
        state["prediction"] = prediction
        state["termination"] = termination
        
        return state
    
    async def stream(self, query: str, context_id: str) -> AsyncIterable[Dict[str, Any]]:
        """
        æµå¼è°ƒç”¨æ™ºèƒ½ä½“ï¼Œé€‚é… A2A åè®®çš„çŠ¶æ€æ ¼å¼
        
        é€šè¿‡æ™ºèƒ½ä½“å¼‚æ­¥æµå¼å¤„ç†æŸ¥è¯¢ï¼Œå¹¶æ ¹æ®ä¸åŒæ‰§è¡Œé˜¶æ®µè¿”å›ä¸åŒçŠ¶æ€çš„å“åº”ã€‚
        
        å‚æ•°:
            query (str): ç”¨æˆ·çš„æŸ¥è¯¢é—®é¢˜
            context_id (str): å¯¹è¯ä¸Šä¸‹æ–‡IDï¼Œç”¨äºä¿æŒä¼šè¯è¿ç»­æ€§
            
        è¿”å›:
            AsyncIterable[Dict[str, Any]]: å¼‚æ­¥è¿­ä»£å™¨ï¼Œäº§ç”ŸåŒ…å«çŠ¶æ€å’Œå†…å®¹çš„å­—å…¸
            å¯èƒ½çš„çŠ¶æ€åŒ…æ‹¬:
                - "working": Agent æ­£åœ¨æ€è€ƒæˆ–æ‰§è¡Œå·¥å…·è°ƒç”¨
                - "completed": å¤„ç†å®Œæˆï¼ŒåŒ…å«æœ€ç»ˆç­”æ¡ˆ
                - "failed": å¤„ç†å¤±è´¥
        
        å¼‚å¸¸:
            æ•è·æ‰€æœ‰å¼‚å¸¸å¹¶ä»¥å¤±è´¥çŠ¶æ€è¿”å›é”™è¯¯ä¿¡æ¯
        """
        try:
            # æ„å»º system prompt
            system_prompt = SYSTEM_PROMPT
            cur_date = datetime.now().strftime("%Y-%m-%d")
            system_prompt = system_prompt + str(cur_date)
            
            # åˆå§‹åŒ–çŠ¶æ€ï¼ŒåŒ…å« system prompt
            initial_state = {
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query}
                ],
                "question": query,
                "answer": "",
                "prediction": "",
                "termination": "",
                "round": 0,
                "num_llm_calls_available": MAX_LLM_CALL_PER_RUN,
                "start_time": time.time(),
                "tool_name": None,
                "tool_args": None,
                "tool_result": None,
                "should_terminate": False,
                "termination_reason": None,
                "need_final_answer": False,
            }
            
            # é…ç½®ï¼šè®¾ç½®é€’å½’é™åˆ¶å’Œçº¿ç¨‹ID
            config = {
                "recursion_limit": 150,
                "configurable": {"thread_id": context_id}
            }
            
            # ç¬¬ä¸€ä¸ªçŠ¶æ€æ›´æ–°ï¼šå¼€å§‹å¤„ç†
            yield {
                "status": "working",
                "content": "å¼€å§‹å¤„ç†æ‚¨çš„è¯·æ±‚..."
            }
            
            # ä½¿ç”¨ astream å¼‚æ­¥æµå¼å¤„ç†
            async for state in self.graph.astream(initial_state, config=config, stream_mode="values"):

                # è·å–å½“å‰çŠ¶æ€çš„æœ€æ–°æ¶ˆæ¯
                if "messages" not in state or len(state["messages"]) == 0:
                    continue
                
                latest_message = state["messages"][-1]
                content = latest_message.get("content", "")
                role = latest_message.get("role", "")
                
                # æ ¹æ®æ¶ˆæ¯ç±»å‹è¿”å›ä¸åŒçŠ¶æ€
                if state.get("prediction"):
                    # å·²æå–æœ€ç»ˆç­”æ¡ˆï¼ˆä¼˜å…ˆæ£€æŸ¥ï¼Œå› ä¸ºè¿™æ˜¯æœ€ç»ˆçŠ¶æ€ï¼‰
                    yield {
                        "status": "completed",
                        "content": state["prediction"]
                    }
                
                elif role == "assistant" and "<tool_call>" in content:
                    # LLM ç”Ÿæˆäº†å·¥å…·è°ƒç”¨
                    tool_name = state.get("tool_name")
                    
                    if tool_name:
                        # Parse èŠ‚ç‚¹å·²å®Œæˆï¼Œæ˜¾ç¤ºå…·ä½“å·¥å…·åç§°
                        yield {
                            "status": "working",
                            "content": f"æ­£åœ¨è°ƒç”¨å·¥å…·ã€{tool_name}ã€‘..."
                        }
                    else:
                        # LLM èŠ‚ç‚¹åˆšå®Œæˆï¼Œè¿˜æœªè§£æ
                        yield {
                            "status": "working",
                            "content": "æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œæ­£åœ¨è§£æ..."
                        }
                
                elif role == "user" and "<tool_response>" in content:
                    # å·¥å…·è¿”å›ç»“æœ
                    yield {
                        "status": "working",
                        "content": "å·¥å…·æ‰§è¡Œå®Œæˆï¼Œæ­£åœ¨å¤„ç†ç»“æœ..."
                    }
                    
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            print(f"Agent æ‰§è¡Œé”™è¯¯: {error_details}")
            yield {
                "status": "failed",
                "content": f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
            }